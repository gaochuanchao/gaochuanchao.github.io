name: Get Citation Data

on:
  # Trigger daily
  schedule:
    - cron: '0 0 * * *'
  # Trigger manually
  workflow_dispatch:
  # Optional: Trigger after Pages site is deployed (if needed)
  workflow_run:
    workflows: ["Pages Build and Deployment"]
    types:
      - completed

env:
  Email: ${{ vars.EMAIL }}

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      Email: ${{ vars.EMAIL }}
      GOOGLE_SCHOLAR_ID: ${{ secrets.GOOGLE_SCHOLAR_ID }}
    steps:
    - uses: actions/checkout@v3

    - name: Install Python and pip
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-pip python3-setuptools

    - name: Run Crawler Script
      run: |
        set -e
        cd ./google_scholar_crawler
        pip3 install -r requirements.txt
        python3 main.py

    - name: Push Citation Data to Pages Branch
      run: |
        cd ./google_scholar_crawler/results

        # Clone pages branch (e.g., gh-pages) or use google-scholar-stats if separate
        git init
        git config user.name "${GITHUB_ACTOR}"
        git config user.email "${Email}"
        git add *.json
        git commit -m "Update citation data"
        git push "https://${GITHUB_ACTOR}:${{ secrets.GITHUB_TOKEN }}@github.com/${GITHUB_REPOSITORY}.git" HEAD:google-scholar-stats --force
      env:
        GOOGLE_SCHOLAR_ID: ${{ secrets.GOOGLE_SCHOLAR_ID }}
